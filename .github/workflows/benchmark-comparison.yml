name: Benchmark Comparison

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0
  # XLA compilation cache - keyed by OS and architecture to avoid mismatches
  # This significantly speeds up JAX compilation (4-10x on complex models)
  JAX_COMPILATION_CACHE_DIR: /tmp/jax-xla-cache

jobs:
  benchmark:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: standard
            benchmarks: "rc,graetz,mul,ring,tb_dp"
            run_tests: true
            timeout: 60
          - name: c6288
            benchmarks: "c6288"
            run_tests: false
            timeout: 60
    name: benchmark (${{ matrix.name }})
    timeout-minutes: ${{ matrix.timeout }}
    steps:
      - name: Checkout with submodules
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            cmake ninja-build \
            flex bison libfl-dev \
            libsuitesparse-dev \
            ccache \
            bc

      - name: Install LLVM 18
        run: |
          wget https://apt.llvm.org/llvm.sh
          chmod +x llvm.sh
          sudo ./llvm.sh 18
          # Install additional LLVM tools needed by OpenVAF
          sudo apt-get install -y lld-18
          # Add LLVM tools to PATH and set environment
          echo "/usr/lib/llvm-18/bin" >> "$GITHUB_PATH"
          echo "LLVM_SYS_181_PREFIX=/usr/lib/llvm-18" >> "$GITHUB_ENV"

      - name: Set up Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache ccache
        uses: actions/cache@v4
        with:
          path: ~/.ccache
          key: ccache-${{ runner.os }}-${{ github.sha }}
          restore-keys: |
            ccache-${{ runner.os }}-

      - name: Configure ccache
        run: |
          ccache --set-config=max_size=500M
          ccache --set-config=compression=true
          ccache -z  # Zero stats

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: |
            openvaf_jax/openvaf_py
            vendor/OpenVAF

      - name: Install uv
        uses: astral-sh/setup-uv@v4

      - name: Set up Python
        run: uv python install 3.11

      # Cache XLA compiled artifacts to speed up JAX compilation
      # Key includes OS, arch, and JAX version to avoid binary incompatibilities
      - name: Cache XLA compilation artifacts
        uses: actions/cache@v4
        with:
          path: /tmp/jax-xla-cache
          key: xla-cache-${{ runner.os }}-${{ runner.arch }}-${{ matrix.name }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            xla-cache-${{ runner.os }}-${{ runner.arch }}-${{ matrix.name }}-
            xla-cache-${{ runner.os }}-${{ runner.arch }}-

      - name: Build openvaf-py
        working-directory: openvaf_jax/openvaf_py
        run: |
          uv sync
          uv run maturin develop --release

      - name: Build openvaf-r (standalone compiler for VACASK)
        working-directory: vendor/OpenVAF
        run: |
          # Build openvaf-r binary for OSDI compilation
          # Use --no-default-features --features llvm18 to build with LLVM 18 (default is LLVM 21)
          cargo build --release -p openvaf-driver --bin openvaf-r --no-default-features --features llvm18
          echo "OPENVAF_DIR=${{ github.workspace }}/vendor/OpenVAF/target/release" >> "$GITHUB_ENV"

      - name: Install va-jax
        run: uv sync --extra dev

      - name: Run VA-JAX tests
        if: matrix.run_tests
        env:
          JAX_PLATFORMS: cpu
        run: uv run pytest tests/test_vacask_suite.py tests/test_vacask_jax.py -v --tb=short

      # Build VACASK simulator
      - name: Cache Boost 1.88
        uses: actions/cache@v4
        id: cache-boost
        with:
          path: /tmp/boost_1_88_0
          key: boost-1.88.0-${{ runner.os }}

      - name: Download and build Boost 1.88
        if: steps.cache-boost.outputs.cache-hit != 'true'
        run: |
          cd /tmp
          wget -q https://archives.boost.io/release/1.88.0/source/boost_1_88_0.tar.gz
          tar -xzf boost_1_88_0.tar.gz
          cd boost_1_88_0
          ./bootstrap.sh --with-libraries=filesystem,process,system
          ./b2 -j"$(nproc)" link=static

      - name: Download toml++ 3.4.0
        run: |
          cd /tmp
          wget -q https://github.com/marzer/tomlplusplus/archive/refs/tags/v3.4.0.tar.gz
          tar -xzf v3.4.0.tar.gz

      - name: Cache VACASK build
        uses: actions/cache@v4
        id: cache-vacask
        with:
          path: vendor/VACASK/build
          # Include OpenVAF sources since openvaf-r changes affect OSDI compilation
          key: vacask-build-${{ runner.os }}-${{ hashFiles('vendor/VACASK/CMakeLists.txt', 'vendor/VACASK/lib/**', 'vendor/VACASK/simulator/**', 'vendor/VACASK/devices/**', 'vendor/OpenVAF/Cargo.lock') }}

      - name: Build VACASK
        if: steps.cache-vacask.outputs.cache-hit != 'true'
        run: |
          cd vendor/VACASK
          mkdir -p build && cd build
          cmake .. \
            -G Ninja \
            -DCMAKE_BUILD_TYPE=Release \
            -DCMAKE_C_COMPILER_LAUNCHER=ccache \
            -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \
            -DBoost_NO_SYSTEM_PATHS=TRUE \
            -DBoost_INCLUDE_DIR=/tmp/boost_1_88_0 \
            -DCMAKE_PREFIX_PATH="/tmp/boost_1_88_0/stage/lib/cmake" \
            -DTOMLPP_DIR=/tmp/tomlplusplus-3.4.0 \
            -DOPENVAF_DIR="${OPENVAF_DIR}"
          # Build simulator and OSDI device files (parallel)
          ninja -j"$(nproc)" vacask
          # Try to build device OSDI files (may fail on some platforms)
          ninja -j"$(nproc)" build_and_stage_devices || {
            echo "Warning: OSDI device compilation failed"
            echo "=== OpenVAF crash logs ==="
            cat /tmp/openvaf-crash-*.log 2>/dev/null || echo "No crash logs found"
          }
          # Show ccache stats
          echo "=== ccache stats ==="
          ccache -s

      # Run automated VACASK result comparison tests (standard benchmarks only)
      # These tests compare simulation waveforms between VA-JAX and VACASK
      # Tests marked with xfail are expected to fail and won't break CI
      - name: Run VACASK result comparison tests
        if: matrix.run_tests
        timeout-minutes: 90
        env:
          JAX_PLATFORMS: cpu
          JAX_ENABLE_X64: "1"
          VACASK_BIN: ${{ github.workspace }}/vendor/VACASK/build/simulator/vacask
          # Limit steps in CI to avoid timeout (full 1M steps takes too long)
          VA_JAX_MAX_STEPS: "10000"
          # Disable progress callbacks to allow XLA compilation caching
          VA_JAX_NO_PROGRESS: "1"
          # Enable JAX compilation tracing to debug CI slowdowns
          JAX_LOG_COMPILES: "1"
          JAX_EXPLAIN_CACHE_MISSES: "1"
        run: |
          echo "Running automated VACASK result comparison tests..."
          echo "Note: Limiting to 10k steps per benchmark for CI (full run uses up to 1M)"
          # Run pytest with the VACASK comparison tests
          # Tests marked xfail won't cause failure, but unexpected failures will
          # Using --tb=long for full tracebacks, --log-cli-level=INFO for timing
          uv run pytest tests/test_vacask_benchmarks.py -v \
            -k "TestVACASKResultComparison" \
            --tb=long --log-cli-level=INFO 2>&1 | tee /tmp/vacask-result-tests.log

      # Summarize XLA compilation times for CI debugging
      - name: Summarize XLA compilation times
        if: always() && matrix.run_tests
        run: |
          echo "=== XLA Compilation Summary ==="
          echo "Top 20 slowest XLA compilations:"
          grep "Finished XLA compilation" /tmp/vacask-result-tests.log 2>/dev/null | \
            sed 's/.*of jit(\([^)]*\)) in \([0-9.]*\) sec.*/\2 \1/' | \
            sort -rn | head -20 || echo "No XLA compilation data found"
          echo ""
          echo "Total compilations: $(grep -c "Finished XLA compilation" /tmp/vacask-result-tests.log 2>/dev/null || echo 0)"
          echo "Cache hits: $(grep -c "Persistent compilation cache hit" /tmp/vacask-result-tests.log 2>/dev/null || echo 0)"
          echo "Cache misses: $(grep -c "PERSISTENT COMPILATION CACHE MISS" /tmp/vacask-result-tests.log 2>/dev/null || echo 0)"

      # Run benchmark performance comparison
      - name: Run benchmark performance comparison
        env:
          JAX_PLATFORMS: cpu
          JAX_ENABLE_X64: "1"
          VA_JAX_NO_PROGRESS: "1"
          JAX_LOG_COMPILES: "1"
          JAX_EXPLAIN_CACHE_MISSES: "1"
        run: |
          echo "Running VA-JAX vs VACASK comparison (${{ matrix.name }})..."
          echo "Benchmarks: ${{ matrix.benchmarks }}"
          uv run python scripts/compare_vacask.py \
            --benchmark ${{ matrix.benchmarks }} \
            --use-scan \
            --use-sparse \
            --json-output /tmp/benchmark-summary.json 2>&1 | tee /tmp/benchmark-comparison.log || true

      - name: Check for JIT recompilation
        if: always()
        run: |
          if [ -f /tmp/benchmark-comparison.log ]; then
            # Count total compilations in the benchmark run
            TOTAL=$(grep -c "Finished XLA compilation" /tmp/benchmark-comparison.log 2>/dev/null || echo 0)
            echo "Total XLA compilations: $TOTAL"

            # Filter to simulation-critical functions (exclude JAX primitives like add, multiply, etc.)
            # JAX primitives recompile with different shapes during setup â€” this is expected.
            # We only care about our own simulation functions recompiling unexpectedly.
            SIMULATION_FNS=$(grep "Finished XLA compilation" /tmp/benchmark-comparison.log 2>/dev/null | \
              sed 's/.*of jit(\([^)]*\)).*/\1/' | \
              grep -E '(full_mna|nr_solve|scan_body|step_fn|transient|simulate|warmup|_solve_step|_time_step)' || true)

            if [ -n "$SIMULATION_FNS" ]; then
              UNIQUE_SIM=$(echo "$SIMULATION_FNS" | sort -u | wc -l | tr -d ' ')
              TOTAL_SIM=$(echo "$SIMULATION_FNS" | wc -l | tr -d ' ')
              RECOMPILES=$((TOTAL_SIM - UNIQUE_SIM))

              echo "Simulation function compilations: $TOTAL_SIM (unique: $UNIQUE_SIM)"

              if [ "$RECOMPILES" -gt 0 ]; then
                echo "::warning::Detected $RECOMPILES simulation function recompilation(s)"
                echo "Recompiled simulation functions:"
                echo "$SIMULATION_FNS" | sort | uniq -c | sort -rn | \
                  awk '$1 > 1 {print "  " $1 "x: " $2}'
              else
                echo "No simulation function recompilations detected"
              fi
            else
              echo "No simulation function compilations found in log"
            fi
          fi

      # Report XLA compilation cache status
      - name: Report XLA cache status
        if: always()
        env:
          JAX_PLATFORMS: cpu
          JAX_ENABLE_X64: "1"
        run: |
          echo "=== XLA Compilation Cache Status ==="
          echo "Cache directory: ${JAX_COMPILATION_CACHE_DIR}"

          if [ -d "${JAX_COMPILATION_CACHE_DIR}" ]; then
            CACHE_SIZE=$(du -sh "${JAX_COMPILATION_CACHE_DIR}" 2>/dev/null | cut -f1 || echo "empty")
            CACHE_FILES=$(find "${JAX_COMPILATION_CACHE_DIR}" -type f 2>/dev/null | wc -l || echo 0)
            echo "Cache size: ${CACHE_SIZE}"
            echo "Cached artifacts: ${CACHE_FILES}"
          else
            echo "Cache directory not found (first run or cache miss)"
          fi

      # Generate comparison summary for GitHub step output
      - name: Generate comparison summary
        if: always()
        run: |
          {
            echo "## Benchmark Comparison: ${{ matrix.name }}"
            echo ""
            echo "### Environment"
            echo "- **Platform**: CPU (GitHub Actions runner)"
            echo "- **JAX Backend**: cpu"
            echo "- **Mode**: lax.scan (JIT-compiled)"
            echo "- **Benchmarks**: ${{ matrix.benchmarks }}"
            echo ""

            # Report VACASK result comparison tests (standard only)
            if [ -f "/tmp/vacask-result-tests.log" ]; then
              echo "### Result Accuracy (vs VACASK)"
              echo ""
              echo "| Benchmark | Solver | RMS Error | Threshold | Status |"
              echo "|-----------|--------|-----------|-----------|--------|"

              # Parse log: "BENCH comparison summary:" followed by "  solver: X.XX% (max allowed: Y%)"
              # Use awk to track current benchmark name
              awk '
                /comparison summary:/ {
                  # Extract benchmark name (uppercase, before "comparison")
                  match($0, /([A-Z0-9_]+) comparison summary/, arr)
                  if (arr[1] != "") bench = tolower(arr[1])
                }
                /: [0-9]+\.[0-9]+% \(max allowed: [0-9]+%\)/ {
                  # Extract solver, rms, threshold
                  match($0, /([a-z]+): ([0-9.]+)% \(max allowed: ([0-9]+)%\)/, arr)
                  if (arr[1] != "") {
                    printf "| %s | %s | %s%% | %s%% | PASS |\n", bench, arr[1], arr[2], arr[3]
                    bench = ""  # Only show benchmark name on first line
                  }
                }
              ' /tmp/vacask-result-tests.log

              # Also show xfail/skip/fail status for benchmarks
              grep -oP 'test_transient_matches_vacask\[\K[^\]]+' /tmp/vacask-result-tests.log | sort -u | while read -r bench; do
                if grep -q "test_transient_matches_vacask\[${bench}\].*XFAIL" /tmp/vacask-result-tests.log; then
                  echo "| ${bench} | - | - | - | XFAIL |"
                elif grep -q "test_transient_matches_vacask\[${bench}\].*FAILED" /tmp/vacask-result-tests.log; then
                  echo "| ${bench} | - | - | - | FAILED |"
                fi
              done

              echo ""
            fi

            # Extract and display the comparison output
            if [ -f "/tmp/benchmark-comparison.log" ]; then
              # Check if VACASK was available and working
              if grep -q "VACASK binary: NOT FOUND" /tmp/benchmark-comparison.log; then
                echo "### VACASK Binary Not Found"
                echo ""
                echo "VACASK simulator binary was not built. Check the Build VACASK step."
                echo ""
              elif grep -q "\.osdi' not found" /tmp/benchmark-comparison.log; then
                echo "### VACASK OSDI Files Missing"
                echo ""
                echo "VACASK binary exists but OSDI device files are missing."
                echo "openvaf-r OSDI compilation fails on Linux/LLVM18 (known upstream issue)."
                echo "VA-JAX benchmarks ran successfully - showing VA-JAX only results."
                echo ""
                echo "For full comparison results, run locally on macOS."
                echo ""
              fi

              echo "### Per-Step Timing"
              echo ""
              echo "| Benchmark | Steps | VA-JAX (ms/step) | VACASK (ms/step) | Ratio |"
              echo "|-----------|-------|---------------------|------------------|-------|"

              # Parse the per-step summary table from the log
              grep -A 10 "Summary (per-step timing)" /tmp/benchmark-comparison.log | \
                grep "^|" | tail -n +3 | while read -r line; do
                echo "$line"
              done

              echo ""
              echo "### Total Wall Time"
              echo ""
              echo "| Benchmark | Steps | VA-JAX (ms) | VACASK (ms) | Ratio |"
              echo "|-----------|-------|----------------|-------------|-------|"

              # Parse the total time summary table from the log
              grep -A 10 "Summary (total wall time)" /tmp/benchmark-comparison.log | \
                grep "^|" | tail -n +3 | while read -r line; do
                echo "$line"
              done

              echo ""
              echo "### Interpretation"
              echo ""
              echo "- **Ratio < 1.0**: VA-JAX is faster"
              echo "- **Ratio > 1.0**: VA-JAX is slower (overhead-dominated for small circuits)"
              echo "- **N/A**: VACASK comparison not available"
            else
              echo "**Error**: Benchmark comparison log not found"
            fi
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Upload benchmark logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-logs-${{ matrix.name }}
          path: |
            /tmp/benchmark-comparison.log
            /tmp/vacask-result-tests.log

      - name: Upload benchmark summary JSON
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-summary-${{ matrix.name }}
          path: /tmp/benchmark-summary.json
          if-no-files-found: ignore
